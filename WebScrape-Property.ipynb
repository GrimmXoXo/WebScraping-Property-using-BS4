{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4105fccc",
      "metadata": {
        "id": "4105fccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6536c94b-d181-4a67-ecf9-6c04974519f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "import time\n",
        "from google.colab import files\n",
        "from collections import deque\n",
        "!pip install requests_cache\n",
        "import requests_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f1f766",
      "metadata": {
        "id": "46f1f766"
      },
      "outputs": [],
      "source": [
        "condition=True\n",
        "while condition:\n",
        "  try:\n",
        "    city=input('Enter City Name \\n')\n",
        "    if 2<=len(city)<=50 and city.isalpha():\n",
        "      condition=False\n",
        "    else:\n",
        "      print('Please input a valid City')\n",
        "  except:\n",
        "    continue\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List contents of My Drive directory\n",
        "drive_path = '/content/drive/My Drive/WebScraping Project' #can change the drive path as needed\n",
        "os.listdir(drive_path)\n",
        "# /content/drive/MyDrive/WebScraping Project/proxy_extract.py"
      ],
      "metadata": {
        "id": "VxQ0g8FLONKX"
      },
      "id": "VxQ0g8FLONKX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9VcuEcUzi1eE",
      "metadata": {
        "id": "9VcuEcUzi1eE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f'{drive_path}')\n",
        "from proxy_extract import _extract_proxies_free_proxy_list_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67795de",
      "metadata": {
        "id": "a67795de"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'authority': 'www.99acres.com',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'no-cache',\n",
        "    'dnt': '1',\n",
        "    'pragma': 'no-cache',\n",
        "    'referer': f'https://www.99acres.com/flats-in-{city}-ffid-page',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"macOS\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'Expect':'',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requests_cache.install_cache(f'{drive_path}/my_cache', expire_after=3600)#Create a cache that will store the runtime information and you wont have to load anything even if your runtime ends"
      ],
      "metadata": {
        "id": "PMqMJ45mFiZj"
      },
      "id": "PMqMJ45mFiZj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main Script**\n",
        "\n",
        "*   Handled Exceptions Correctly\n",
        "*   Used sessions to clear cookies\n",
        "*   Used Proxy Mesh to create multiple Proxies to reduce chances of IP ban\n",
        "*   Handling Requests in such a way that if they throw exception, the loop will break but the page will restart from where they should be before the error\n",
        "*   Use Placeholder to Handle Null data for raw as well as data structures\n",
        "*   Used O(n) time complexity\n",
        "*   Created a TimeOut such that if there is a delay in the response header, the script will handle it\n",
        "*   The Mesh of proxies will refesh ones the whole pool is completed and the pool has currently 300 Proxies but i wont recommend it to use for sensitive data as it is open proxies which has security issues\n",
        "*   Handled the HTTP header and exceptions such that if error occurs we know what caused the error\n",
        "\n"
      ],
      "metadata": {
        "id": "9IEyFYKvtj_u"
      },
      "id": "9IEyFYKvtj_u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bDTyxTRNPUs",
      "metadata": {
        "id": "0bDTyxTRNPUs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import requests\n",
        "request=0\n",
        "Placeholder=None\n",
        "\n",
        "location=[]\n",
        "\n",
        "All_Facilities=[]\n",
        "Area=[]\n",
        "Address=[]\n",
        "NearbyLocations=[]\n",
        "Description=[]\n",
        "Features=[]\n",
        "Builder_Name=[]\n",
        "Building_Name=[]\n",
        "Rating=[]\n",
        "Property_id=[]\n",
        "property_data =[]\n",
        "location_advantages = []\n",
        "project_details = {}\n",
        "\n",
        "\n",
        "\n",
        "proxies=_extract_proxies_free_proxy_list_net(https=False)\n",
        "P_300=[]\n",
        "for proxy_obj in proxies:\n",
        "\n",
        "    if proxy_obj['https']=='yes':\n",
        "        proxy_obj['protocol']='https'\n",
        "    else:\n",
        "        proxy_obj['protocol']='http'\n",
        "    proxy = f\"{proxy_obj['protocol']}://{proxy_obj['ip']}:{proxy_obj['port']}\"\n",
        "    P_300.append(proxy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract proxies and create a queue with 9 repetitions for each proxy\n",
        "def create_proxy_queue(proxies):\n",
        "    proxy_queue = deque()\n",
        "    for proxy in proxies:\n",
        "        for _ in range(9):  # Repeat each proxy 9 times\n",
        "            proxy_queue.append(proxy)\n",
        "    return proxy_queue\n",
        "\n",
        "\n",
        "# Create a queue of proxies with 9 repetitions for each proxy\n",
        "proxy_queue = create_proxy_queue(P_300)\n",
        "\n",
        "end=2589\n",
        "\n",
        "# Load the value of j from the file if it exists this value will be k for next iteration\n",
        "try:\n",
        "    with open(f'{drive_path}/j_value.txt', 'r') as file:\n",
        "        k = int(file.read().strip())\n",
        "except FileNotFoundError:\n",
        "    # Handle the case when the file doesn't exist (first run)\n",
        "    k = 1  # Set a default value for k if needed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in range(k,end):\n",
        "#     time.sleep(1)\n",
        "    # Get the next proxy from the queue\n",
        "    proxy = proxy_queue.popleft()\n",
        "    session=requests.Session()\n",
        "    links=[]\n",
        "\n",
        "    try:\n",
        "        url=f\"https://www.99acres.com/flats-in-{city}-ffid-page-{j}\"\n",
        "\n",
        "        r=session.get(url,headers=headers,proxies={'http':proxy},timeout=5)\n",
        "        r.raise_for_status()\n",
        "        pageSoup = BeautifulSoup(r.content, 'html.parser')\n",
        "        request+=1\n",
        "        for page in pageSoup.find_all('div',class_=\"projectTuple__descCont\"):\n",
        "          try:\n",
        "            links.append(page.a['href'])\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Detail Page -Link start\n",
        "        for link in links:\n",
        "\n",
        "            try:\n",
        "               page = session.get(link, headers=headers,proxies={'http':proxy},timeout=5)\n",
        "               dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
        "              #  time.sleep(random.uniform(1,3))\n",
        "               request += 1\n",
        "            except requests.exceptions.RequestException as e:\n",
        "               print(f\"An error occurred while making the request: {e}\")\n",
        "               continue\n",
        "\n",
        "            property_info = {\n",
        "                \"Rating\": Placeholder,\n",
        "                \"BHK Types\": [],\n",
        "                \"Property Types\": [],\n",
        "                \"Area Ranges (sq.ft.)\": [],\n",
        "                \"Price Ranges (Cr)\": [],\n",
        "                \"Additional Charges\": [],\n",
        "                \"Construction Status\" : [],\n",
        "                \"Completion Date\":[]\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "            # Rating by Features- Property Data\n",
        "            try:\n",
        "                rating = [i.text for i in dpageSoup.select_one('div.review__rightSide>div>ul>li>div').select('div.ratingByFeature__circleWrap')]\n",
        "            except AttributeError:\n",
        "                rating=None\n",
        "            property_info[\"Rating\"] = rating\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Price Information BHK wise-Property Info->Property data\n",
        "            all_id = ['1_1', '1_2', '1_3', '1_4', '1_5']\n",
        "            for id in all_id:\n",
        "                soup = dpageSoup.find('div', id=id)\n",
        "                try:\n",
        "                    bhk_type = soup.select_one('.configurationCards__configBandLabel').text.strip()\n",
        "                except AttributeError:\n",
        "                    bhk_type = Placeholder\n",
        "                try:\n",
        "                    property_type = soup.select_one('.configurationCards__configBandHeading').text.strip()\n",
        "                except AttributeError:\n",
        "                    property_type = Placeholder\n",
        "                try:\n",
        "                    area_range_text = soup.select_one('.configurationCards__cardAreaSubHeadingOne').text.strip()\n",
        "                except AttributeError:\n",
        "                    area_range_text = Placeholder\n",
        "                try:\n",
        "                    price_range_text = soup.select_one('.configurationCards__cardPriceHeading').text.strip()\n",
        "                except AttributeError:\n",
        "                    price_range_text = Placeholder\n",
        "                try:\n",
        "                    additional_charges_element = soup.select_one('.configurationCards__adtnCharges')\n",
        "                    additional_charges = additional_charges_element.text.strip('+ ').strip() if additional_charges_element else Placeholder\n",
        "                except AttributeError:\n",
        "                    additional_charges = Placeholder\n",
        "\n",
        "                property_info[\"BHK Types\"].append(bhk_type)\n",
        "                property_info[\"Property Types\"].append(property_type)\n",
        "                property_info[\"Area Ranges (sq.ft.)\"].append(area_range_text)\n",
        "                property_info[\"Price Ranges (Cr)\"].append(price_range_text)\n",
        "                property_info[\"Additional Charges\"].append(additional_charges)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Construction Status-property info-> Property Data\n",
        "\n",
        "\n",
        "\n",
        "            # Find the div with class ConstructionStatus__collapsedCard\n",
        "            construction_status_div = dpageSoup.find('div', class_='ConstructionStatus__collapsedCard')\n",
        "\n",
        "            # Initialize construction status and completion date with Placeholder\n",
        "            construction_status = Placeholder\n",
        "            completion_date = Placeholder\n",
        "\n",
        "            try:\n",
        "                # Extract construction status and completion date if the div is found\n",
        "                construction_status_element = construction_status_div.select_one('.ConstructionStatus__phaseStatus')\n",
        "                completion_date_element = construction_status_div.select_one('.ConstructionStatus__phaseStatusSubtitle')\n",
        "\n",
        "                construction_status = construction_status_element.text.strip() if construction_status_element else Placeholder\n",
        "                completion_date = completion_date_element.text.strip() if completion_date_element else Placeholder\n",
        "\n",
        "            except Exception as e:\n",
        "                # print(f\"An error occurred while extracting construction status and completion date: {e}\")\n",
        "                pass\n",
        "\n",
        "                # Continue with Placeholder values\n",
        "\n",
        "            # Add construction status and completion date to the property_data dictionary\n",
        "            property_info[\"Construction Status\"].append(construction_status)\n",
        "            property_info[\"Completion Date\"].append(completion_date)\n",
        "\n",
        "\n",
        "            property_data.append(property_info)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #Facilities- All_Facilities\n",
        "            try:\n",
        "              Facility_info=dpageSoup.find_all('div',class_='UniquesFacilities__xidFacilitiesCard')\n",
        "              Facilities=[i.text.strip() for i in Facility_info]\n",
        "            except:\n",
        "              Facilities=Placeholder\n",
        "            All_Facilities.append(Facilities)\n",
        "\n",
        "            #Builder Info\n",
        "\n",
        "            # try:\n",
        "            #   Builder_info=dpageSoup.find('div',class_=\"ProjectInfo__imgBox1 title_bold\").text.strip()\n",
        "            # except:\n",
        "            #   Builder_info=Placeholder\n",
        "\n",
        "\n",
        "\n",
        "            #Outer Details- location and Building_Name\n",
        "            try:\n",
        "                Outer_Details = dpageSoup.find('h1', class_='ProjectInfo__imgBox1 title_bold')\n",
        "                location_text = Outer_Details.find('span', class_='ProjectInfo__hideTxt').text.strip()\n",
        "                building_name = Outer_Details.contents[0].strip()\n",
        "            except AttributeError:\n",
        "                # Handle the exception if any of the elements are not found\n",
        "                location_text = Placeholder\n",
        "                building_name = Placeholder\n",
        "\n",
        "            location.append(location_text)\n",
        "            Building_Name.append(building_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Extracting project details - property_data\n",
        "            try:\n",
        "                project_details_div = dpageSoup.find('div', class_='AboutProjectDetail__specTable caption_strong_medium font_family')\n",
        "                project_details_table = project_details_div.find('table', class_='AboutProjectDetail__specificTable')\n",
        "                rows = project_details_table.find_all('tr')\n",
        "\n",
        "                # Create a new dictionary for project details\n",
        "                project_details = {key: Placeholder for key in [\"Towers\", \"Floors\", \"Units\", \"Total Project Area\", \"Open Area\"]}\n",
        "\n",
        "                for row in rows:\n",
        "                    columns = row.find_all('td')\n",
        "                    if len(columns) == 2:\n",
        "                        key = columns[0].text.strip()\n",
        "                        value = columns[1].text.strip()\n",
        "                        project_details[key] = value\n",
        "            except AttributeError:\n",
        "                # Handle the exception if the specified elements are not found\n",
        "                project_details = {key: Placeholder for key in [\"Towers\", \"Floors\", \"Units\", \"Total Project Area\", \"Open Area\"]}\n",
        "\n",
        "            # Append project details to the DataFrame\n",
        "            property_data.append(project_details)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #Builder Name- Builder_Name\n",
        "            try:\n",
        "              name=dpageSoup.find('div',class_='section_header_bold spacer4').text.strip()\n",
        "            except:\n",
        "              name=Placeholder\n",
        "            Builder_Name.append(name)\n",
        "\n",
        "\n",
        "            #Nearby Locations-location_advantages\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Loop through each div tag and extract Location and Time\n",
        "            for loc in dpageSoup.find_all('div', class_='locAdvantageCarousel__locAdBoxDesktop ellipsis'):\n",
        "                try:\n",
        "                    name = loc.find('div', class_='list_header_semiBold').text.strip()\n",
        "                except AttributeError:\n",
        "                    name = Placeholder  # Handle if name is not found\n",
        "                try:\n",
        "                    number = loc.find('div', class_='caption_subdued_medium').text.strip()\n",
        "                except AttributeError:\n",
        "                    number = Placeholder  # Handle if number is not found\n",
        "\n",
        "                # Create a dictionary and append to the list\n",
        "                location_dict = {\n",
        "                    \"Location_Near\": name,\n",
        "                    \"Time_taken\": number\n",
        "                }\n",
        "                location_advantages.append(location_dict)\n",
        "\n",
        "\n",
        "\n",
        "            #Description-Description of each link\n",
        "            try:\n",
        "              Description.append(dpageSoup.find('div', {'data-label': 'PROJ_DESC'}).text.strip())\n",
        "            except:\n",
        "              Description.append(Placeholder)\n",
        "\n",
        "        print(f'We are currently at: {j} page')\n",
        "\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "      if errh.response.status_code == 417:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "        break\n",
        "\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"Request Exception: {err}\")\n",
        "\n",
        "     # If the proxy queue is empty, refill it for the next set of requests\n",
        "    if not proxy_queue:\n",
        "        proxy_queue = create_proxy_queue(proxies)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the value of j to a file\n",
        "with open(f'{drive_path}/j_value.txt', 'w') as file:\n",
        "    file.write(str(j))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N29ScBlozQMz"
      },
      "id": "N29ScBlozQMz",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the whole code into csv which will be connected to the drive and concatinating the previous csv to the new csv, if previous Csv doesn't exist then create a new one and then append the items in that one\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEWDfB3Zqvpj"
      },
      "id": "qEWDfB3Zqvpj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c165f1",
      "metadata": {
        "id": "d5c165f1"
      },
      "outputs": [],
      "source": [
        "All=[location,location_advantages,All_Facilities,Description,Builder_Name,Building_Name,property_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1dc1a9b",
      "metadata": {
        "id": "d1dc1a9b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Function to load existing data from CSV if it exists\n",
        "def load_existing_data(csv_filepath):\n",
        "    try:\n",
        "        # Try to read the CSV file from the specified file path\n",
        "        return pd.read_csv(csv_filepath)\n",
        "    except FileNotFoundError:\n",
        "      return pd.DataFrame(columns=[\"All Facilities\", \"Location\", \"Building Name\", \"Builder Name\",\n",
        "                                     \"Location Advantages\", \"Description\", \"Property Data\"])\n",
        "\n",
        "# Load existing data from CSV into Main DataFrame\n",
        "\n",
        "filepath = f'{drive_path}/data.csv'\n",
        "Main = load_existing_data(filepath)\n",
        "\n",
        "# List of lists to iterate through\n",
        "lists_to_iterate = [All_Facilities, location, Building_Name, Builder_Name, location_advantages, Description, property_data]\n",
        "\n",
        "# List of dictionaries to store property data\n",
        "property_data_list = []\n",
        "\n",
        "# Iterate through the lists and create property dictionaries\n",
        "for data_points in zip(*lists_to_iterate):\n",
        "    property_dict = {\n",
        "        \"All Facilities\": data_points[0],\n",
        "        \"Location\": data_points[1],\n",
        "        \"Building Name\": data_points[2],\n",
        "        \"Builder Name\": data_points[3],\n",
        "        \"Location Advantages\": data_points[4],\n",
        "        \"Description\": data_points[5],\n",
        "        'Property Data': data_points[6]\n",
        "    }\n",
        "    property_data_list.append(property_dict)\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "new_data = pd.DataFrame(property_data_list)\n",
        "\n",
        "# Concatenate Main DataFrame and new_data vertically\n",
        "Main = pd.concat([Main, new_data], ignore_index=True)\n",
        "\n",
        "# Write the updated Main DataFrame to the CSV file\n",
        "Main.to_csv(filepath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d66c53",
      "metadata": {
        "id": "73d66c53"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LsuJz4fvvkuW"
      },
      "id": "LsuJz4fvvkuW"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IU9Y7YOgvTFt"
      },
      "id": "IU9Y7YOgvTFt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093e122b",
      "metadata": {
        "id": "093e122b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yo98bAXutp7r"
      },
      "id": "yo98bAXutp7r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCDyrnJOyuto"
      },
      "id": "HCDyrnJOyuto",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tTa0c7vhyzqd"
      },
      "id": "tTa0c7vhyzqd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qEWDfB3Zqvpj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}